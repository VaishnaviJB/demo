{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c8ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0188f827",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa5e9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-utils\n",
      "  Downloading keras-utils-1.0.13.tar.gz (2.4 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Keras>=2.1.5 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from keras-utils) (2.14.0)\n",
      "Building wheels for collected packages: keras-utils\n",
      "  Building wheel for keras-utils (setup.py): started\n",
      "  Building wheel for keras-utils (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-utils: filename=keras_utils-1.0.13-py3-none-any.whl size=2658 sha256=bb1cb40f4acfa5b31e910d4d4aca9487e7fd2ae2fbb87389a2af265e8c234f0b\n",
      "  Stored in directory: c:\\users\\vaish\\appdata\\local\\pip\\cache\\wheels\\84\\04\\c8\\f3d21e09aa3a1e25bc4a4fc07341ca073d7372f33dbd344a06\n",
      "Successfully built keras-utils\n",
      "Installing collected packages: keras-utils\n",
      "Successfully installed keras-utils-1.0.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7216521b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting np_utils\n",
      "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
      "     ---------------------------------------- 0.0/62.0 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.5/62.0 kB 330.3 kB/s eta 0:00:01\n",
      "     ------------------------------- ------ 51.2/62.0 kB 525.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 62.0/62.0 kB 556.4 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from np_utils) (1.24.3)\n",
      "Building wheels for collected packages: np_utils\n",
      "  Building wheel for np_utils (setup.py): started\n",
      "  Building wheel for np_utils (setup.py): finished with status 'done'\n",
      "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56454 sha256=f969fd93a332ed524bde9a635be2190998143de73f1bd95a89f4edf20e958918\n",
      "  Stored in directory: c:\\users\\vaish\\appdata\\local\\pip\\cache\\wheels\\19\\0d\\33\\eaa4dcda5799bcbb51733c0744970d10edb4b9add4f41beb43\n",
      "Successfully built np_utils\n",
      "Installing collected packages: np_utils\n",
      "Successfully installed np_utils-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6feaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a5f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c192905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: future in f:\\users\\vaish\\anaconda3\\lib\\site-packages (0.18.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install future\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "350a4885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting enum34\n",
      "  Downloading enum34-1.1.10-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: enum34\n",
      "Successfully installed enum34-1.1.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install enum34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Assuming X_train, y_train, X_val, and y_val are your data\n",
    "# Assuming number of classes is the number of classes in your problem\n",
    "\n",
    "# Define y_train and y_val\n",
    "y_train = tf.to_categorical(y_train, num_classes=number_of_classes)\n",
    "y_val = tf.to_categorical(y_val, num_classes=number_of_classes)\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(number_of_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5721b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "# ... (your other imports)\n",
    "\n",
    "# Assuming X_train, y_train, X_val, and y_val are your data\n",
    "# Assuming number_of_classes is the number of classes in your problem\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train,num_classes=number_of_classes)\n",
    "y_val = to_categorical(y_val,num_classes=number_of_classes)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "# ... (define your model architecture)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cce4ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv(\"C://Users//vaish//train.csv\")\n",
    "test = pd.read_csv(\"C://Users//vaish//test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train[\"label\"]\n",
    "\n",
    "# Drop 'label' column\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "# free some space\n",
    "del train \n",
    "\n",
    "g = sns.countplot(Y_train)\n",
    "\n",
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab2ee7",
   "metadata": {},
   "source": [
    "# 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1a0270",
   "metadata": {},
   "source": [
    "# Check for null and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0ef2e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data\n",
    "X_train.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf9c84d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b06a7",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "076e0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "test = test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55946a48",
   "metadata": {},
   "source": [
    "# Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f83730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19856b1a",
   "metadata": {},
   "source": [
    "# Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20f77518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "Y_train = to_categorical(Y_train, num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27f4c9",
   "metadata": {},
   "source": [
    "# Split training and valdiation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c47e304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random_seed = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d15c70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and the validation set for the fitting\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "473d3cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some examples\n",
    "g = plt.imshow(X_train[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09551c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in f:\\users\\vaish\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow in f:\\users\\vaish\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.59.2)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade keras tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a867d",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "501c1743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Users\\vaish\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Users\\vaish\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Users\\vaish\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        25632     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               803072    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 885217 (3.38 MB)\n",
      "Trainable params: 885217 (3.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Set the CNN model \n",
    "# my CNN architecture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='Same', activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='Same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same', activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594059ae",
   "metadata": {},
   "outputs": [],
   "source": [
    " pip install MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99f68f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in f:\\users\\vaish\\anaconda3\\lib\\site-packages (2.14.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.14.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.25.0)\n",
      "Requirement already satisfied: setuptools in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.59.2)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.14.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52180dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = RMSprop(lr=0.001,rho=0.9,epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09781645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2473f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db141d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "batch_size = 86"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c7d39f",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28df1bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without data augmentation i obtained an accuracy of 0.98114\n",
    "#history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n",
    "#          validation_data = (X_val, Y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d667418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With data augmentation to prevent overfitting (accuracy 0.99286)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0a617",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves for training and validation \n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at confusion matrix \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb760ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some error results \n",
    "\n",
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_val_errors = X_val[errors]\n",
    "\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n",
    "\n",
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f5d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a model that predicts classes and the true classes are in Y_true\n",
    "# Replace this with your actual model prediction and true labels\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1)  # Assuming a classification task with one-hot encoded predictions\n",
    "Y_true = np.argmax(Y_test, axis=1)  # Assuming a classification task with one-hot encoded true labels\n",
    "\n",
    "# Display some error results\n",
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "\n",
    "# Now you can use Y_pred_classes_errors, Y_pred_errors, and Y_true_errors for further analysis or visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some error results \n",
    "\n",
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_val_errors = X_val[errors]\n",
    "\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n",
    "\n",
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cf5ab75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement Y_pred_classes (from versions: none)\n",
      "ERROR: No matching distribution found for Y_pred_classes\n"
     ]
    }
   ],
   "source": [
    "pip install Y_pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b259a147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Users\\vaish\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Users\\vaish\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Users\\vaish\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Users\\vaish\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From F:\\Users\\vaish\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Users\\vaish\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1500/1500 [==============================] - 17s 11ms/step - loss: 0.1692 - accuracy: 0.9499 - val_loss: 0.0651 - val_accuracy: 0.9808\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 16s 11ms/step - loss: 0.0567 - accuracy: 0.9827 - val_loss: 0.0574 - val_accuracy: 0.9831\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 17s 11ms/step - loss: 0.0362 - accuracy: 0.9891 - val_loss: 0.0482 - val_accuracy: 0.9852\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 17s 11ms/step - loss: 0.0239 - accuracy: 0.9922 - val_loss: 0.0473 - val_accuracy: 0.9855\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 16s 10ms/step - loss: 0.0168 - accuracy: 0.9947 - val_loss: 0.0498 - val_accuracy: 0.9862\n",
      "313/313 [==============================] - 2s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(X_train, y_train), (X_test, _) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)  # Assuming 10 classes for digits 0-9\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a simple CNN model (replace this with your actual model architecture)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))  # Assuming 10 classes for digits 0-9\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train.reshape(-1, 28, 28, 1), y_train, epochs=5, validation_data=(X_val.reshape(-1, 28, 28, 1), y_val))\n",
    "\n",
    "# Now, assuming you have a test dataset (replace this with your actual test data loading code)\n",
    "# For example, assuming you have a DataFrame named 'test_data' with features\n",
    "# test = test_data  # Features\n",
    "\n",
    "# Preprocess the test data (replace this with your actual test data preprocessing code)\n",
    "# For example, assuming you have a DataFrame named 'test_data' with features\n",
    "# test = test_data  # Features\n",
    "# test = test.astype('float32') / 255.0  # Normalize pixel values\n",
    "\n",
    "# Predict results using the trained model\n",
    "results = model.predict(X_test.reshape(-1, 28, 28, 1))\n",
    "\n",
    "# Select the index with the maximum probability\n",
    "results = np.argmax(results, axis=1)\n",
    "\n",
    "# Assuming you have ImageId in your test data, you can create a submission DataFrame\n",
    "submission = pd.concat([pd.Series(range(1, len(X_test) + 1), name=\"ImageId\"), pd.Series(results, name=\"Label\")], axis=1)\n",
    "\n",
    "# Save the submission to a CSV file\n",
    "submission.to_csv(\"cnn_mnist_datagen.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b686b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cdb0b4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# predict results\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# select the indix with the maximum probability\u001b[39;00m\n\u001b[0;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(results,axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# predict results\n",
    "results = model.predict(test)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec567a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
    "\n",
    "submission.to_csv(\"cnn_mnist_datagen.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4404b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'results' with appropriate data\n",
    "# For example, you might have a column named 'Label' containing the predicted labels\n",
    "\n",
    "# Creating a dummy DataFrame for illustration\n",
    "results_data = {\n",
    "    'Label': [0, 1, 2, 3, ...]  # Replace this with your actual predicted labels\n",
    "}\n",
    "\n",
    "results = pd.DataFrame(results_data)\n",
    "\n",
    "submission = pd.concat([pd.Series(range(1, 28001), name=\"ImageId\"), results], axis=1)\n",
    "submission.to_csv(\"cnn_mnist_datagen.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50a86906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'results' with appropriate data\n",
    "# For example, you might have a column named 'Label' containing the predicted labels\n",
    "\n",
    "# Creating a dummy DataFrame for illustration\n",
    "results_data = {\n",
    "    'Label': [0, 1, 2, 3, ...]  # Replace this with your actual predicted labels\n",
    "}\n",
    "\n",
    "results = pd.DataFrame(results_data)\n",
    "\n",
    "submission = pd.concat([pd.Series(range(1, 28001), name=\"ImageId\"), results], axis=1)\n",
    "submission.to_csv(\"cnn_mnist_datagen.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e47d0",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789cef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleCNN(num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ba8a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  # Add this line\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(14 * 14 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Usage example\n",
    "num_classes = 10\n",
    "\n",
    "# Create an instance of the model\n",
    "model = SimpleCNN(num_classes)\n",
    "\n",
    "# The rest of your code (loss function, optimizer, training loop, etc.) goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c8a20b",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30c32aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/938], Loss: 0.3740\n",
      "Epoch [1/5], Step [200/938], Loss: 0.1055\n",
      "Epoch [1/5], Step [300/938], Loss: 0.1147\n",
      "Epoch [1/5], Step [400/938], Loss: 0.2102\n",
      "Epoch [1/5], Step [500/938], Loss: 0.0346\n",
      "Epoch [1/5], Step [600/938], Loss: 0.0227\n",
      "Epoch [1/5], Step [700/938], Loss: 0.0373\n",
      "Epoch [1/5], Step [800/938], Loss: 0.0235\n",
      "Epoch [1/5], Step [900/938], Loss: 0.1193\n",
      "Epoch [2/5], Step [100/938], Loss: 0.0877\n",
      "Epoch [2/5], Step [200/938], Loss: 0.0157\n",
      "Epoch [2/5], Step [300/938], Loss: 0.0422\n",
      "Epoch [2/5], Step [400/938], Loss: 0.0162\n",
      "Epoch [2/5], Step [500/938], Loss: 0.0244\n",
      "Epoch [2/5], Step [600/938], Loss: 0.0375\n",
      "Epoch [2/5], Step [700/938], Loss: 0.0173\n",
      "Epoch [2/5], Step [800/938], Loss: 0.0201\n",
      "Epoch [2/5], Step [900/938], Loss: 0.0036\n",
      "Epoch [3/5], Step [100/938], Loss: 0.0198\n",
      "Epoch [3/5], Step [200/938], Loss: 0.0165\n",
      "Epoch [3/5], Step [300/938], Loss: 0.0911\n",
      "Epoch [3/5], Step [400/938], Loss: 0.0047\n",
      "Epoch [3/5], Step [500/938], Loss: 0.0415\n",
      "Epoch [3/5], Step [600/938], Loss: 0.0059\n",
      "Epoch [3/5], Step [700/938], Loss: 0.0030\n",
      "Epoch [3/5], Step [800/938], Loss: 0.0614\n",
      "Epoch [3/5], Step [900/938], Loss: 0.0015\n",
      "Epoch [4/5], Step [100/938], Loss: 0.0603\n",
      "Epoch [4/5], Step [200/938], Loss: 0.0228\n",
      "Epoch [4/5], Step [300/938], Loss: 0.0041\n",
      "Epoch [4/5], Step [400/938], Loss: 0.0045\n",
      "Epoch [4/5], Step [500/938], Loss: 0.0450\n",
      "Epoch [4/5], Step [600/938], Loss: 0.0453\n",
      "Epoch [4/5], Step [700/938], Loss: 0.0079\n",
      "Epoch [4/5], Step [800/938], Loss: 0.0119\n",
      "Epoch [4/5], Step [900/938], Loss: 0.0091\n",
      "Epoch [5/5], Step [100/938], Loss: 0.0282\n",
      "Epoch [5/5], Step [200/938], Loss: 0.0054\n",
      "Epoch [5/5], Step [300/938], Loss: 0.0288\n",
      "Epoch [5/5], Step [400/938], Loss: 0.0086\n",
      "Epoch [5/5], Step [500/938], Loss: 0.0761\n",
      "Epoch [5/5], Step [600/938], Loss: 0.0004\n",
      "Epoch [5/5], Step [700/938], Loss: 0.0711\n",
      "Epoch [5/5], Step [800/938], Loss: 0.0006\n",
      "Epoch [5/5], Step [900/938], Loss: 0.0012\n",
      "Test Accuracy: 99.11%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleCNN(num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd575371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8fe66f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
