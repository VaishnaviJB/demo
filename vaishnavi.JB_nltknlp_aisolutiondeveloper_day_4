{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f094e9a",
   "metadata": {},
   "source": [
    "# Create Your Own Spell Checker\n",
    "\n",
    "Objective: Creating a spell checker, correct the incorrect word in the given sentence.\n",
    "\n",
    "Problem Statement: While typing or sending any message to person, we generally make spelling mistakes. Write a script which will correct the misspelled words in a sentence. The input will be a raw string and the output will be a string with the case normalized and the incorrect word corrected.\n",
    "\n",
    "Domain: General\n",
    "\n",
    "Analysis to be done: Words availability in corpus\n",
    "\n",
    "Content:\n",
    "\n",
    "Dataset: None\n",
    "\n",
    "We will be using NLTK's inbuilt corpora (words, stop words etc.) and no specific dataset.\n",
    "\n",
    "Steps to perform:\n",
    "\n",
    "While there are several approaches to correct spelling, you will use the Levenshtein or Edit distance approach.\n",
    "\n",
    "The approach will be straightforward for correcting a word:\n",
    "\n",
    "If the word is present in a list of valid words, the word is correct.\n",
    "\n",
    "If the word is absent from the valid word list, we will find the correct word, i.e., the word from the valid word list which has the lowest edit distance from the target word.\n",
    "\n",
    "Once you define a function, you will iterate over the terms in the given sentence, correct the words identified as incorrect, and return a joined string with all the terms. To help speed up execution, you won't be applying the spell check on the stop words and punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f6765",
   "metadata": {},
   "source": [
    "# 1. Get a list of valid words in the English language using NLTK's list of words (Hint: use nitk.download('words') to get the raw list.\n",
    "\n",
    "2. Look at the first 20 words in the list. Is the casing normalized?\n",
    "\n",
    "3. Normalize the casing for all the terms.\n",
    "\n",
    "4. Some duplicates would have been induced, create unique list after normalizing.\n",
    "\n",
    "5. Create a list of stop words which should include:\n",
    "\n",
    "i. Stop words from NLTK\n",
    "\n",
    "ii. All punctuations (Hint: use 'punctuation' from string module)\n",
    "\n",
    "iii. Final list should be a combination of these two\n",
    "\n",
    "6. Define a function to get correct a single term\n",
    "\n",
    "For a given term, find its edit distance with each term in the valid word list. To speed up execution, you can use the first 20,000 entries in the valid word list.\n",
    "\n",
    "Store the result in a dictionary, the key as the term, and edit distance as value.\n",
    "\n",
    "Sort the dictionary in ascending order of the values.\n",
    "\n",
    "Return the first entry in the sorted result (value with minimum edit distance).\n",
    "\n",
    "Using the function, get the correct word for committee.\n",
    "\n",
    "7. Make a set from the list of valid words, for faster lookup to see if word is in valid list or not.\n",
    "\n",
    "8. Define a function for spelling correction in any given input sentence:\n",
    "\n",
    "1. To tokenize them after making all the terms in lowercase For each term in the tokenized sentence:\n",
    "\n",
    "2. Check if the term is in the list of valid words (valid_words_set).\n",
    "\n",
    "3. If yes, return the word as is.\n",
    "\n",
    "4. If no, get the correct word using get_correct_term function.\n",
    "\n",
    "5. To return the joined string as output.\n",
    "\n",
    "9. Test the function for the input sentence \"The new abacos is great\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2df7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in f:\\users\\vaish\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b00eab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b134a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Specify a different NLTK data server\n",
    "nltk.data.path.append(\"/path/to/nltk_data\")\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4f5b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package wordnet2022 to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet2022.zip.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "from nltk import wsd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from spacy.cli import download\n",
    "from spacy import load\n",
    "import warnings\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet2022')\n",
    "\n",
    "\n",
    "! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet # temp fix for lookup error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb4d638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the list: ['Phalangiidae', 'afterings', 'Shortzy', 'barful', 'dancery', 'alphabetically', 'incus', 'nonreference', 'lovelass', 'Siphonognathus', 'archspirit', 'tricosylic', 'xanthaline', 'mishandle', 'septicolored', 'scouch', 'nonmillionaire', 'roughings', 'isoclimatic', 'harden']\n",
      "Correct word for 'committee': committee\n",
      "Corrected Sentence: the new acarus is great\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_valid_words():\n",
    "    valid_words = set(words.words())\n",
    "    return valid_words\n",
    "\n",
    "def normalize_casing(word_list):\n",
    "    return [word.lower() for word in word_list]\n",
    "\n",
    "def get_unique_list(word_list):\n",
    "    return list(set(word_list))\n",
    "\n",
    "def get_stop_words():\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    stop_words.update(set(string.punctuation))\n",
    "    return stop_words\n",
    "\n",
    "def get_correct_term(target_term, valid_words_list):\n",
    "    distances = {word: edit_distance(target_term, word) for word in valid_words_list}\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[1])\n",
    "    return sorted_distances[0][0]\n",
    "\n",
    "def create_valid_words_set(valid_words_list):\n",
    "    return set(valid_words_list)\n",
    "\n",
    "def correct_spelling(input_sentence, valid_words_set):\n",
    "    tokenized_sentence = nltk.word_tokenize(input_sentence.lower())\n",
    "\n",
    "    corrected_sentence = []\n",
    "    for term in tokenized_sentence:\n",
    "        if term in valid_words_set or term in stop_words:\n",
    "            corrected_sentence.append(term)\n",
    "        else:\n",
    "            corrected_term = get_correct_term(term, list(valid_words_set)[:20000])\n",
    "            corrected_sentence.append(corrected_term)\n",
    "\n",
    "    return ' '.join(corrected_sentence)\n",
    "\n",
    "# Step 1: Get a list of valid words\n",
    "valid_words_list = list(get_valid_words())\n",
    "\n",
    "# Step 2: Look at the first 20 words in the list\n",
    "print(\"First 20 words in the list:\", valid_words_list[:20])\n",
    "\n",
    "# Step 3: Normalize the casing for all the terms\n",
    "valid_words_list = normalize_casing(valid_words_list)\n",
    "\n",
    "# Step 4: Create a unique list after normalizing\n",
    "valid_words_list = get_unique_list(valid_words_list)\n",
    "\n",
    "# Step 5: Create a list of stop words\n",
    "stop_words = get_stop_words()\n",
    "\n",
    "# Step 6: Define a function to get correct a single term\n",
    "correct_word_committee = get_correct_term(\"committee\", valid_words_list)\n",
    "print(\"Correct word for 'committee':\", correct_word_committee)\n",
    "\n",
    "# Step 7: Make a set from the list of valid words\n",
    "valid_words_set = create_valid_words_set(valid_words_list)\n",
    "\n",
    "# Step 8: Define a function for spelling correction\n",
    "input_sentence = \"The new abacos is great\"\n",
    "corrected_sentence = correct_spelling(input_sentence, valid_words_set)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ff4cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet2022 to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet2022 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet2022')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48ec1d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awe anew abacus as areal\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words, stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Get a list of valid words\n",
    "valid_words = set(words.words()[:20000])  # Using the first 20,000 entries for speed\n",
    "\n",
    "# Step 2-4: Normalize casing and create a unique list\n",
    "valid_words = set(word.lower() for word in valid_words)\n",
    "\n",
    "# Step 5: Create a list of stop words\n",
    "stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "# Step 6: Define a function to get the correct term\n",
    "def get_correct_term(term):\n",
    "    term_lower = term.lower()\n",
    "    distances = {word: nltk.edit_distance(term_lower, word) for word in valid_words}\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[1])\n",
    "    return sorted_distances[0][0]\n",
    "\n",
    "# Step 7: Make a set from the list of valid words\n",
    "valid_words_set = set(valid_words)\n",
    "\n",
    "# Step 8: Define a function for spelling correction in any given input sentence\n",
    "def correct_spelling(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())  # Tokenize and normalize casing\n",
    "    corrected_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in valid_words_set:\n",
    "            corrected_tokens.append(token)\n",
    "        else:\n",
    "            corrected_tokens.append(get_correct_term(token))\n",
    "\n",
    "    return ' '.join(corrected_tokens)\n",
    "\n",
    "# Step 9: Test the function\n",
    "input_sentence = \"The new abacos is great\"\n",
    "output_sentence = correct_spelling(input_sentence)\n",
    "print(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94986a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import jaccard_distance\n",
    "\n",
    "# Calculate the Jaccard similarity between two words\n",
    "def calculate_jaccard_similarity(word1, word2):\n",
    "    return 1 - jaccard_distance(set(word1), set(word2))\n",
    "\n",
    "# Get the correct term using the Jaccard similarity as a distance metric\n",
    "def get_correct_term(term):\n",
    "    term_lower = term.lower()\n",
    "    distances = {word: calculate_jaccard_similarity(term_lower, word) for word in valid_words}\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_distances[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "182d89ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the list: ['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron', 'Aaronic', 'Aaronical', 'Aaronite', 'Aaronitic', 'Aaru', 'Ab', 'aba', 'Ababdeh', 'Ababua', 'abac']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words\n",
    "valid_words = nltk.corpus.words.words()\n",
    "\n",
    "# Print the first 20 words in the list\n",
    "print(\"First 20 words in the list:\", valid_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d99ba862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a\n",
      "aa\n",
      "aal\n",
      "aalii\n",
      "aam\n",
      "aani\n",
      "aardvark\n",
      "aardwolf\n",
      "aaron\n",
      "aaronic\n",
      "aaronical\n",
      "aaronite\n",
      "aaronitic\n",
      "aaru\n",
      "ab\n",
      "aba\n",
      "ababdeh\n",
      "ababua\n",
      "abac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words and normalize the casing\n",
    "valid_words = [word.lower() for word in nltk.corpus.words.words()]\n",
    "\n",
    "# Print the first 20 words in the list with normalized casing\n",
    "for word in valid_words[:20]:\n",
    " print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df973eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the list with normalized casing: ['a', 'a', 'aa', 'aal', 'aalii', 'aam', 'aani', 'aardvark', 'aardwolf', 'aaron', 'aaronic', 'aaronical', 'aaronite', 'aaronitic', 'aaru', 'ab', 'aba', 'ababdeh', 'ababua', 'abac']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words and normalize the casing\n",
    "valid_words = [word.lower() for word in nltk.corpus.words.words()]\n",
    "\n",
    "# Print the first 20 words in the list with normalized casing\n",
    "print(\"First 20 words in the list with normalized casing:\", valid_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f7aad0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the unique list with normalized casing: ['afterings', 'barful', 'dancery', 'dendrobatinae', 'alphabetically', 'saulteur', 'incus', 'nonreference', 'lovelass', 'archspirit', 'tricosylic', 'xanthaline', 'mishandle', 'septicolored', 'scouch', 'nonmillionaire', 'mesomyodi', 'isoclimatic', 'roughings', 'harden']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words and normalize the casing\n",
    "valid_words = [word.lower() for word in nltk.corpus.words.words()]\n",
    "\n",
    "# Create a unique list after normalizing casing\n",
    "unique_valid_words = list(set(valid_words))\n",
    "\n",
    "# Print the first 20 words in the unique list\n",
    "print(\"First 20 words in the unique list with normalized casing:\", unique_valid_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09f4ac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the final list of stop words: ['should', '\\\\', \"needn't\", 'are', 'yourselves', 'their', 'those', 'ours', 'can', \"couldn't\", \"you're\", 'couldn', \"hasn't\", 'he', 'weren', \"'\", 'didn', '+', 'further', 'too']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "# Download the NLTK stop words if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of stop words from NLTK\n",
    "stop_words_nltk = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Get all punctuations from the string module\n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "# Create the final list of stop words by combining the two sets\n",
    "stop_words_final = stop_words_nltk.union(punctuations)\n",
    "\n",
    "# Print the first 20 words in the final list\n",
    "print(\"First 20 words in the final list of stop words:\", list(stop_words_final)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf333482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct word for 'committee': commutate\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "def get_correct_term(target_term, valid_words_list):\n",
    "    # Use the first 20,000 entries in the valid word list\n",
    "    valid_words_list = valid_words_list[:20000]\n",
    "\n",
    "    # Store the edit distances in a dictionary\n",
    "    edit_distances = {word: edit_distance(target_term, word) for word in valid_words_list}\n",
    "\n",
    "    # Sort the dictionary in ascending order of edit distances\n",
    "    sorted_distances = sorted(edit_distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Return the first entry in the sorted result (minimum edit distance)\n",
    "    return sorted_distances[0][0]\n",
    "\n",
    "# Example: Get the correct word for 'committee'\n",
    "correct_word_committee = get_correct_term(\"committee\", valid_words_list)\n",
    "print(\"Correct word for 'committee':\", correct_word_committee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94731c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'example' is in the valid words list.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words and normalize the casing\n",
    "valid_words_list = [word.lower() for word in nltk.corpus.words.words()]\n",
    "\n",
    "# Create a set from the list of valid words for faster lookup\n",
    "valid_words_set = set(valid_words_list)\n",
    "\n",
    "# Example: Check if a word is in the valid words set\n",
    "word_to_check = 'example'\n",
    "if word_to_check in valid_words_set:\n",
    "    print(f\"'{word_to_check}' is in the valid words list.\")\n",
    "else:\n",
    "    print(f\"'{word_to_check}' is not in the valid words list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70ad1744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: the new acarus is great\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "def get_valid_words_set():\n",
    "    # Get the list of valid English words and normalize the casing\n",
    "    valid_words_list = [word.lower() for word in nltk.corpus.words.words()]\n",
    "    # Create a set from the list of valid words for faster lookup\n",
    "    return set(valid_words_list)\n",
    "\n",
    "def get_correct_term(target_term, valid_words_list):\n",
    "    # Use the first 20,000 entries in the valid word list\n",
    "    valid_words_list = valid_words_list[:20000]\n",
    "\n",
    "    # Store the edit distances in a dictionary\n",
    "    edit_distances = {word: edit_distance(target_term, word) for word in valid_words_list}\n",
    "\n",
    "    # Sort the dictionary in ascending order of edit distances\n",
    "    sorted_distances = sorted(edit_distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Return the first entry in the sorted result (minimum edit distance)\n",
    "    return sorted_distances[0][0]\n",
    "\n",
    "def correct_spelling(input_sentence, valid_words_set):\n",
    "    # Tokenize the input sentence after making all terms lowercase\n",
    "    tokenized_sentence = nltk.word_tokenize(input_sentence.lower())\n",
    "\n",
    "    # Correct spelling for each term in the tokenized sentence\n",
    "    corrected_sentence = [term if term in valid_words_set else get_correct_term(term, list(valid_words_set)) for term in tokenized_sentence]\n",
    "\n",
    "    # Return the joined string as output\n",
    "    return ' '.join(corrected_sentence)\n",
    "\n",
    "# Example usage:\n",
    "input_sentence = \"The new abacos is great\"\n",
    "valid_words_set = get_valid_words_set()\n",
    "corrected_sentence = correct_spelling(input_sentence, valid_words_set)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d21f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: The new abacos is great\n",
      "Corrected Sentence: the new acarus is great\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "def get_valid_words_set():\n",
    "    # Get the list of valid English words and normalize the casing\n",
    "    valid_words_list = [word.lower() for word in nltk.corpus.words.words()]\n",
    "    # Create a set from the list of valid words for faster lookup\n",
    "    return set(valid_words_list)\n",
    "\n",
    "def get_correct_term(target_term, valid_words_list):\n",
    "    # Use the first 20,000 entries in the valid word list\n",
    "    valid_words_list = valid_words_list[:20000]\n",
    "\n",
    "    # Store the edit distances in a dictionary\n",
    "    edit_distances = {word: edit_distance(target_term, word) for word in valid_words_list}\n",
    "\n",
    "    # Sort the dictionary in ascending order of edit distances\n",
    "    sorted_distances = sorted(edit_distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Return the first entry in the sorted result (minimum edit distance)\n",
    "    return sorted_distances[0][0]\n",
    "\n",
    "def correct_spelling(input_sentence, valid_words_set):\n",
    "    # Tokenize the input sentence after making all terms lowercase\n",
    "    tokenized_sentence = nltk.word_tokenize(input_sentence.lower())\n",
    "\n",
    "    # Correct spelling for each term in the tokenized sentence\n",
    "    corrected_sentence = [term if term in valid_words_set else get_correct_term(term, list(valid_words_set)) for term in tokenized_sentence]\n",
    "\n",
    "    # Return the joined string as output\n",
    "    return ' '.join(corrected_sentence)\n",
    "\n",
    "# Test the function with the input sentence\n",
    "input_sentence = \"The new abacos is great\"\n",
    "valid_words_set = get_valid_words_set()\n",
    "corrected_sentence = correct_spelling(input_sentence, valid_words_set)\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414bdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
