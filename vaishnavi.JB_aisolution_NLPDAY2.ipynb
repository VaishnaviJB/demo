{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfb3a63",
   "metadata": {},
   "source": [
    "# You are part of a team developing a text classification system for a news aggregator platform. The platform aims to categorize news articles into different topics automatically. The dataset contains news articles along with their corresponding topics. Perform only the Feature extraction techniques.\n",
    "\n",
    "Dataset Link: https://www.kaggle.com/datasets/therohk/million-headlines\n",
    "\n",
    "Data Exploration: Begin by exploring the dataset. What are the different topics/categories present in the dataset? What is the distribution of articles across these topics?\n",
    "\n",
    "Bag-of-Words (BoW): Implement a Bag-of-Words (BoW) model using CountVectorizer or TF-IDF to transform the text data into numerical features. Discuss the advantages and limitations of Bow in this context. Apply both unigram and bigram techniques and compare their effects on classification accuracy.\n",
    "\n",
    "N-grams: Explore the use of N-grams (bi-grams, tri-grams) in feature engineering. How do different N-gram ranges impact the performance of the classification model?\n",
    "\n",
    "TF-IDF: Apply TF-IDF (Term Frequency-Inverse Document Frequency) to the text data. Describe how TF-IDF works and its significance in capturing the importance of words\n",
    "\n",
    "across documents. Compare the results of TF-IDF with the BoW approach. One-Hot Encoding: Investigate the application of One-Hot Encoding to encode categorical\n",
    "\n",
    "variables or labels, Can One-Hot Encoding be used directly for text classification? Why or\n",
    "\n",
    "why not?\n",
    "\n",
    "Deliverables:\n",
    "\n",
    "Present insights gathered from data exploration and discuss the impact of different feature engineering techniques (BoW. N-grams, TF-IDF, One-Hot Encoding). Provide recommendations for the best feature engineering strategy. Code in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "039cee09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of the Dataset:\n",
      "   publish_date                                      headline_text\n",
      "0      20030219  aba decides against community broadcasting lic...\n",
      "1      20030219     act fire witnesses must be aware of defamation\n",
      "2      20030219     a g calls for infrastructure protection summit\n",
      "3      20030219           air nz staff in aust strike for pay rise\n",
      "4      20030219      air nz strike to affect australian travellers\n",
      "\n",
      "Categories present in the dataset:\n",
      "[20030219 20030220 20030221 ... 20211229 20211230 20211231]\n",
      "\n",
      "Distribution of articles across topics:\n",
      "publish_date\n",
      "20120824    384\n",
      "20130412    383\n",
      "20110222    380\n",
      "20120814    379\n",
      "20130514    378\n",
      "           ... \n",
      "20210605      6\n",
      "20211023      5\n",
      "20210515      5\n",
      "20210806      1\n",
      "20170209      1\n",
      "Name: count, Length: 6882, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C://Users//vaish//abcnews-date-text.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Sample of the Dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Explore different topics/categories\n",
    "categories = df['publish_date'].unique()\n",
    "print(\"\\nCategories present in the dataset:\")\n",
    "print(categories)\n",
    "\n",
    "# Distribution of articles across topics\n",
    "print(\"\\nDistribution of articles across topics:\")\n",
    "print(df['publish_date'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b282957",
   "metadata": {},
   "source": [
    "# Bag-of-Words (BoW):\n",
    "\n",
    "python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f44375f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in f:\\users\\vaish\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/97/d8/dc2f6bff06a799a5603c414afc6de39c6351fe34892d50b6a077df3be6ac/pandas-2.1.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pandas-2.1.3-cp311-cp311-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.1.3-cp311-cp311-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/10.6 MB 1.2 MB/s eta 0:00:10\n",
      "    --------------------------------------- 0.2/10.6 MB 1.4 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.3/10.6 MB 1.6 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.3/10.6 MB 1.6 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.4/10.6 MB 1.7 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.5/10.6 MB 1.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.6/10.6 MB 1.6 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.6/10.6 MB 1.6 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.7/10.6 MB 1.5 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.8/10.6 MB 1.6 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.9/10.6 MB 1.6 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.0/10.6 MB 1.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.1/10.6 MB 1.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.2/10.6 MB 1.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.3/10.6 MB 1.8 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.5/10.6 MB 1.8 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.5/10.6 MB 1.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.6/10.6 MB 1.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.7/10.6 MB 1.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.8/10.6 MB 1.9 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.9/10.6 MB 1.9 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.9/10.6 MB 1.8 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.0/10.6 MB 1.8 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.0/10.6 MB 1.8 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.1/10.6 MB 1.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.2/10.6 MB 1.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.2/10.6 MB 1.7 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.3/10.6 MB 1.7 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.4/10.6 MB 1.7 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.4/10.6 MB 1.7 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.6/10.6 MB 1.7 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.7/10.6 MB 1.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.7/10.6 MB 1.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.8/10.6 MB 1.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.9/10.6 MB 1.7 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.0/10.6 MB 1.7 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.1/10.6 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.2/10.6 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.3/10.6 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.4/10.6 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 3.5/10.6 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 3.6/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.7/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.8/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.9/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.9/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.0/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.1/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.2/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.3/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.4/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.5/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.5/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.6/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.7/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.7/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 4.8/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 4.9/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.0/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.1/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.1/10.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.2/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.3/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.3/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.3/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.4/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.4/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.5/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.5/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.5/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.5/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.5/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 5.7/10.6 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 5.7/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 5.8/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 5.9/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 5.9/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.0/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.1/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.2/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.2/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.3/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 6.4/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 6.5/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 6.5/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 6.6/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 6.7/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 6.8/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 6.9/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.0/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.1/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.2/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 7.2/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 7.3/10.6 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 7.4/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.5/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.5/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.6/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.7/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.7/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.9/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.0/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.0/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.1/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.2/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.2/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.3/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.3/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.3/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.3/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.3/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.3/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.3/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.6/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.6/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.7/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.8/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.8/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.9/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.0/10.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.1/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.2/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.2/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.3/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.4/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.5/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.5/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.6/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.7/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.8/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.9/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.9/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.0/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.1/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.1/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.2/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.3/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.3/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.4/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.3\n",
      "    Uninstalling pandas-2.0.3:\n",
      "      Successfully uninstalled pandas-2.0.3\n",
      "Successfully installed pandas-2.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "d2l 1.0.3 requires numpy==1.23.5, but you have numpy 1.26.2 which is incompatible.\n",
      "d2l 1.0.3 requires pandas==2.0.3, but you have pandas 2.1.3 which is incompatible.\n",
      "d2l 1.0.3 requires scipy==1.10.1, but you have scipy 1.11.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc884d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b904b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['publish_date', 'headline_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"C://Users//vaish//abcnews-date-text.csv\"  # Replace with the actual path to your downloaded CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check the column names in the DataFrame\n",
    "print(df.columns)\n",
    "\n",
    "# Assuming the column names are 'headline_text' and 'category', adjust accordingly if different\n",
    "# For simplicity, let's focus on a smaller subset for faster execution\n",
    "df = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['headline_text'], df['publish_date'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Implement Bag-of-Words with unigrams using CountVectorizer\n",
    "count_vectorizer_unigram = CountVectorizer()\n",
    "X_train_counts_unigram = count_vectorizer_unigram.fit_transform(X_train)\n",
    "X_test_counts_unigram = count_vectorizer_unigram.transform(X_test)\n",
    "\n",
    "# Implement Bag-of-Words with bigrams using CountVectorizer\n",
    "count_vectorizer_bigram = CountVectorizer(ngram_range=(2, 2))\n",
    "X_train_counts_bigram = count_vectorizer_bigram.fit_transform(X_train)\n",
    "X_test_counts_bigram = count_vectorizer_bigram.transform(X_test)\n",
    "\n",
    "# Implement Bag-of-Words with unigrams using TF-IDF\n",
    "tfidf_vectorizer_unigram = TfidfVectorizer()\n",
    "X_train_tfidf_unigram = tfidf_vectorizer_unigram.fit_transform(X_train)\n",
    "X_test_tfidf_unigram = tfidf_vectorizer_unigram.transform(X_test)\n",
    "\n",
    "# Implement Bag-of-Words with bigrams using TF-IDF\n",
    "tfidf_vectorizer_bigram = TfidfVectorizer(ngram_range=(2, 2))\n",
    "X_train_tfidf_bigram = tfidf_vectorizer_bigram.fit_transform(X_train)\n",
    "X_test_tfidf_bigram = tfidf_vectorizer_bigram.transform(X_test)\n",
    "\n",
    "# Train a simple classifier (e.g., Multinomial Naive Bayes) and evaluate accuracy\n",
    "# (Continue with the classifier and accuracy evaluation as shown in the previous code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8fe1a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['publish_date', 'headline_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3625ff24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publish_date                                      headline_text\n",
      "0      20030219  aba decides against community broadcasting lic...\n",
      "1      20030219     act fire witnesses must be aware of defamation\n",
      "2      20030219     a g calls for infrastructure protection summit\n",
      "3      20030219           air nz staff in aust strike for pay rise\n",
      "4      20030219      air nz strike to affect australian travellers\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fb49fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['headline_text'], df['headline_text'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e37c9ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['publish_date', 'headline_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee514e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a96fe543",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['headline_text'], df['publish_date'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37453b08",
   "metadata": {},
   "source": [
    "Advantages of BoW:\n",
    "\n",
    "Simple and computationally efficient.\n",
    "Captures word frequency information.\n",
    "Limitations of BoW:\n",
    "\n",
    "Ignores word order and structure.\n",
    "Doesn't consider the semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344fd7e",
   "metadata": {},
   "source": [
    "# N-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe027135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca8d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for 2-grams:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"C://Users//vaish//abcnews-date-text.csv\"  # Replace with the actual path to your downloaded CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# For simplicity, let's focus on a smaller subset for faster execution\n",
    "df = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['headline_text'], df['headline_text'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate_model(X_train_features, X_test_features, y_train, y_test):\n",
    "    # Train a Naive Bayes classifier\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_features, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = clf.predict(X_test_features)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Implement N-grams (bi-grams and tri-grams) with CountVectorizer\n",
    "for n in range(2, 4):  # n-grams range from 2 to 3 (bi-grams to tri-grams)\n",
    "    ngram_vectorizer = CountVectorizer(ngram_range=(n, n))\n",
    "    X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
    "    X_test_ngram = ngram_vectorizer.transform(X_test)\n",
    "\n",
    "    print(f'\\nResults for {n}-grams:')\n",
    "    train_and_evaluate_model(X_train_ngram, X_test_ngram, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c768a",
   "metadata": {},
   "source": [
    "Impact of N-grams:\n",
    "\n",
    "Higher-order N-grams capture more contextual information but may lead to increased dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a780b",
   "metadata": {},
   "source": [
    "# TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f26805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with TF-IDF (unigram): 0.0019289503295290146\n",
      "Accuracy with BoW (unigram): 0.003777527728660987\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "dataset_url = \"C://Users//vaish//abcnews-date-text.csv\"\n",
    "df = pd.read_csv(dataset_url)\n",
    "\n",
    "# For simplicity, let's focus on a smaller subset for faster execution\n",
    "df = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['headline_text'], df['publish_date'], test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF with unigrams\n",
    "tfidf_vectorizer_unigram = TfidfVectorizer()\n",
    "X_train_tfidf_unigram = tfidf_vectorizer_unigram.fit_transform(X_train)\n",
    "X_test_tfidf_unigram = tfidf_vectorizer_unigram.transform(X_test)\n",
    "\n",
    "# Bag-of-Words (BoW) with unigrams using CountVectorizer\n",
    "count_vectorizer_unigram = CountVectorizer()\n",
    "X_train_bow_unigram = count_vectorizer_unigram.fit_transform(X_train)\n",
    "X_test_bow_unigram = count_vectorizer_unigram.transform(X_test)\n",
    "\n",
    "# Train a simple classifier (e.g., Multinomial Naive Bayes) and evaluate accuracy\n",
    "\n",
    "# TF-IDF with unigrams\n",
    "model_tfidf_unigram = MultinomialNB()\n",
    "model_tfidf_unigram.fit(X_train_tfidf_unigram, y_train)\n",
    "y_pred_tfidf_unigram = model_tfidf_unigram.predict(X_test_tfidf_unigram)\n",
    "accuracy_tfidf_unigram = accuracy_score(y_test, y_pred_tfidf_unigram)\n",
    "print(f'Accuracy with TF-IDF (unigram): {accuracy_tfidf_unigram}')\n",
    "\n",
    "# BoW with unigrams\n",
    "model_bow_unigram = MultinomialNB()\n",
    "model_bow_unigram.fit(X_train_bow_unigram, y_train)\n",
    "y_pred_bow_unigram = model_bow_unigram.predict(X_test_bow_unigram)\n",
    "accuracy_bow_unigram = accuracy_score(y_test, y_pred_bow_unigram)\n",
    "print(f'Accuracy with BoW (unigram): {accuracy_bow_unigram}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd5e5c7",
   "metadata": {},
   "source": [
    "Significance of TF-IDF:\n",
    "\n",
    "Weights terms based on their importance in the corpus.\n",
    "Captures the uniqueness of words across documents.\n",
    "One-Hot Encoding:\n",
    "One-Hot Encoding is not directly applicable to text data. It is typically used for encoding categorical variables, not sequential data like text. Consider using techniques like BoW, N-grams, or TF-IDF for text classification.\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "TF-IDF often outperforms BoW due to its ability to consider the importance of words.\n",
    "Experiment with different N-gram ranges based on the size of your dataset and the complexity of language patterns.\n",
    "Evaluate multiple models and feature sets to find the best combination for your specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de61b3a",
   "metadata": {},
   "source": [
    "# ONEHOT ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "562bb31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Users\\vaish\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfefb36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "259c4deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in f:\\users\\vaish\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in f:\\users\\vaish\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb27bcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"This is a sample sentence.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e516972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: [('this',), ('is',), ('a',), ('sample',), ('sentence',)]\n",
      "Bigrams: [('this', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'sentence')]\n",
      "Trigrams: [('this', 'is', 'a'), ('is', 'a', 'sample'), ('a', 'sample', 'sentence')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"this is a sample sentence\"\n",
    "\n",
    "# Convert sentence to words\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Create unigrams, bigrams, and trigrams\n",
    "unigrams = list(ngrams(words, 1))\n",
    "bigrams = list(ngrams(words, 2))\n",
    "trigrams = list(ngrams(words, 3))\n",
    "\n",
    "print(\"Unigrams:\",unigrams)\n",
    "print(\"Bigrams:\",bigrams)\n",
    "print(\"Trigrams:\",trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa3813",
   "metadata": {},
   "source": [
    "# one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f4442ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category  Category_A  Category_B  Category_C\n",
      "0        A         1.0         0.0         0.0\n",
      "1        B         0.0         1.0         0.0\n",
      "2        A         1.0         0.0         0.0\n",
      "3        C         0.0         0.0         1.0\n",
      "4        B         0.0         1.0         0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Users\\vaish\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample data with a categorical variable\n",
    "data = {'Category': ['A', 'B', 'A', 'C', 'B']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# One-Hot Encoding\n",
    "encoder = OneHotEncoder(sparse=False)  # Set sparse=False to get a dense array\n",
    "encoded_data = encoder.fit_transform(df[['Category']])\n",
    "\n",
    "# Create a DataFrame with the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['Category']))\n",
    "\n",
    "# Concatenate the original DataFrame with the encoded DataFrame\n",
    "df_encoded = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1069bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample text data\n",
    "text_data = ['This is a positive example', 'This is a negative example', 'Another positive one', 'Negative example here']\n",
    "\n",
    "# Sample labels\n",
    "labels = [10,43, 66, 193]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a classifier (e.g., Naive Bayes)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce48ed4e",
   "metadata": {},
   "source": [
    "# BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5734cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for TF-IDF: 1.0000\n",
      "\n",
      "Accuracy for Bag-of-Words: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"C://Users//vaish//abcnews-date-text.csv\"  # Replace with the actual path\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Create a temporary 'category' column with a placeholder value for demonstration\n",
    "df['category'] = 'placeholder_category'\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['headline_text'], df['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Bag-of-Words (BoW)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Classification using Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Function to train and evaluate the classifier\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, vectorizer_type):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nAccuracy for {vectorizer_type}: {acc:.4f}\")\n",
    "\n",
    "# Evaluate TF-IDF\n",
    "train_and_evaluate(X_train_tfidf, X_test_tfidf, y_train, y_test, \"TF-IDF\")\n",
    "\n",
    "# Evaluate Bag-of-Words\n",
    "train_and_evaluate(X_train_bow, X_test_bow, y_train, y_test,\"Bag-of-Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784bcead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3411b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
